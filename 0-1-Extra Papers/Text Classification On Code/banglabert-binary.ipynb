{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification","metadata":{"id":"2e0f75c3","outputId":"1968999e-09e5-4f8d-fb47-377c1b235879","papermill":{"duration":13.349991,"end_time":"2023-07-25T16:06:48.929213","exception":false,"start_time":"2023-07-25T16:06:35.579222","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:18:02.183939Z","iopub.execute_input":"2023-09-10T23:18:02.184294Z","iopub.status.idle":"2023-09-10T23:18:06.978434Z","shell.execute_reply.started":"2023-09-10T23:18:02.184265Z","shell.execute_reply":"2023-09-10T23:18:06.977459Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"model_name = 'csebuetnlp/banglabert'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# Define device (CPU or GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"id":"4913d932","outputId":"5eb3dd9b-5b04-4803-ae2a-b601b8da7ba0","papermill":{"duration":4.173444,"end_time":"2023-07-25T16:06:53.106774","exception":false,"start_time":"2023-07-25T16:06:48.933330","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:18:06.980350Z","iopub.execute_input":"2023-09-10T23:18:06.980831Z","iopub.status.idle":"2023-09-10T23:18:20.896680Z","shell.execute_reply.started":"2023-09-10T23:18:06.980799Z","shell.execute_reply":"2023-09-10T23:18:20.895533Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cecc258909a941ae9fb92107079419cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a619bbdf3a8a4a1e91415a49392054ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/528k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"722d3f2f16af4bcbad5f1cb1df129dc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cbb06a7d5dc40718c01a01dd898b22f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b0729553b34613ac478e156d1606d5"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ntrain_url = '/kaggle/input/bec-dataset/train_data.csv'\ntest_url = '/kaggle/input/bec-dataset/test_data.csv'\ndf_train = pd.read_csv(train_url)\ndf_test = pd.read_csv(test_url)\nstop_words_df = pd.read_excel('/kaggle/input/bangla-stopwords/stopwords_bangla.xlsx',index_col=False)","metadata":{"id":"b0560ae3","papermill":{"duration":0.132745,"end_time":"2023-07-25T16:06:53.244675","exception":false,"start_time":"2023-07-25T16:06:53.111930","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:18:20.899722Z","iopub.execute_input":"2023-09-10T23:18:20.900420Z","iopub.status.idle":"2023-09-10T23:18:21.433614Z","shell.execute_reply.started":"2023-09-10T23:18:20.900382Z","shell.execute_reply":"2023-09-10T23:18:21.432287Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"STOPWORDS = set([word.strip() for word in stop_words_df['words']])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:18:21.436539Z","iopub.execute_input":"2023-09-10T23:18:21.437384Z","iopub.status.idle":"2023-09-10T23:18:21.448458Z","shell.execute_reply.started":"2023-09-10T23:18:21.437341Z","shell.execute_reply":"2023-09-10T23:18:21.447188Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import re\ndef preprocess(x):\n    html_pattern = re.compile('<.*?>')\n    x = html_pattern.sub(r'', x)\n    x = \" \".join([word for word in str(x).split() if word not in STOPWORDS])\n    return x\ndf_train['Comment'] = df_train['Comment'].apply(lambda x: preprocess(x))\ndf_test['Comment'] = df_test['Comment'].apply(lambda x:preprocess(x))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:18:21.450383Z","iopub.execute_input":"2023-09-10T23:18:21.450773Z","iopub.status.idle":"2023-09-10T23:18:21.514045Z","shell.execute_reply.started":"2023-09-10T23:18:21.450738Z","shell.execute_reply":"2023-09-10T23:18:21.513074Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_no = 5\n\n# Prepare the training data\ntrain_texts = df_train['Comment'].tolist()\ntrain_labels = df_train['Error'].tolist()\n\ntest_texts = df_test['Comment'].tolist()\ntest_labels = df_test['Error'].tolist()","metadata":{"id":"6b9dc3e4","papermill":{"duration":0.018204,"end_time":"2023-07-25T16:06:53.268005","exception":false,"start_time":"2023-07-25T16:06:53.249801","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:18:21.515217Z","iopub.execute_input":"2023-09-10T23:18:21.515542Z","iopub.status.idle":"2023-09-10T23:18:21.524439Z","shell.execute_reply.started":"2023-09-10T23:18:21.515510Z","shell.execute_reply":"2023-09-10T23:18:21.523407Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Tokenize and encode the training texts\ntrain_encodings = tokenizer(train_texts, truncation=True, max_length=128,\n        padding='max_length', return_tensors = 'pt')\n\n# Convert the labels to tensors\ntrain_labels = torch.tensor(train_labels)\n\n# Create a PyTorch dataset\ntrain_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'],\n                                               train_encodings['attention_mask'],\n                                               train_labels)\n\n# Create a data loader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n\nmodel = model.to(device)","metadata":{"id":"c68abf8e","outputId":"fc006e00-bc36-49f7-9041-3cc6a300504b","papermill":{"duration":11.003131,"end_time":"2023-07-25T16:07:04.276102","exception":false,"start_time":"2023-07-25T16:06:53.272971","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:18:21.525654Z","iopub.execute_input":"2023-09-10T23:18:21.526329Z","iopub.status.idle":"2023-09-10T23:18:28.690925Z","shell.execute_reply.started":"2023-09-10T23:18:21.526292Z","shell.execute_reply":"2023-09-10T23:18:28.689837Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\n\n# Set the model to training mode\nmodel.train()\n\n# Define the optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nlosses = []\naccuracies = []  # To store accuracy per epoch\nnum_epochs = 5\n# Training loop\nfor epoch in tqdm(range(num_epochs)):  # Number of training epochs\n    running_loss = 0.0\n    predicted_labels = []  # To store predicted labels for accuracy calculation\n    true_labels = []  # To store true labels for accuracy calculation\n\n    for batch in tqdm(train_loader):\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Convert logits to predicted labels\n        _, predicted = torch.max(logits, dim=1)\n        predicted_labels.extend(predicted.cpu().tolist())\n        true_labels.extend(labels.cpu().tolist())\n\n    epoch_loss = running_loss / len(train_loader)\n    losses.append(epoch_loss)\n\n    # Calculate and store accuracy\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    accuracies.append(accuracy)\n\n    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {accuracy:.4f}')\n\n# Save the model\ntorch.save(model.state_dict(), 'model.pth')","metadata":{"id":"eedb3a9c","outputId":"81f0d82e-5d90-4271-e5cf-71638bf8bc98","papermill":{"duration":668.41198,"end_time":"2023-07-25T16:18:12.693572","exception":false,"start_time":"2023-07-25T16:07:04.281592","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:18:28.692601Z","iopub.execute_input":"2023-09-10T23:18:28.692988Z","iopub.status.idle":"2023-09-10T23:32:50.566283Z","shell.execute_reply.started":"2023-09-10T23:18:28.692954Z","shell.execute_reply":"2023-09-10T23:32:50.565251Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1e9801ff154cab8e0a29577d96ce68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56fe4fbf0d8047a4b8cfd6168ed4d8c4"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/5 - Loss: 0.5292 - Accuracy: 0.7494\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"622dd3874ae146538280f1a75ee2e17b"}},"metadata":{}},{"name":"stdout","text":"Epoch 2/5 - Loss: 0.4176 - Accuracy: 0.8258\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50cbc70738bf45c5881b0808a79a2749"}},"metadata":{}},{"name":"stdout","text":"Epoch 3/5 - Loss: 0.3241 - Accuracy: 0.8767\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03e4e8967558446ebdfcc51e6b840afa"}},"metadata":{}},{"name":"stdout","text":"Epoch 4/5 - Loss: 0.2463 - Accuracy: 0.9132\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/502 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30389acf666048038a7ed3822766dc8f"}},"metadata":{}},{"name":"stdout","text":"Epoch 5/5 - Loss: 0.1780 - Accuracy: 0.9381\n","output_type":"stream"}]},{"cell_type":"code","source":"#dgfdgdfgdgffdgdfd1212jhkhk","metadata":{"papermill":{"duration":0.014424,"end_time":"2023-07-25T16:18:12.713695","exception":false,"start_time":"2023-07-25T16:18:12.699271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:32:50.567700Z","iopub.execute_input":"2023-09-10T23:32:50.568055Z","iopub.status.idle":"2023-09-10T23:32:50.574206Z","shell.execute_reply.started":"2023-09-10T23:32:50.568022Z","shell.execute_reply":"2023-09-10T23:32:50.573311Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\ndef predict_labels(text):\n    train_encodings = tokenizer(text, truncation=True, max_length=128,\n        padding='max_length', return_tensors = 'pt')\n    input_ids = train_encodings['input_ids'].to(device)\n    attention_mask = train_encodings['attention_mask'].to(device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Disable gradient calculation\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    probabilities = torch.softmax(logits, dim=1)\n    predicted_class = torch.argmax(probabilities, dim=1)\n    \n\n    return predicted_class.item(), probabilities[:,1].item()","metadata":{"id":"1243273c","papermill":{"duration":0.69644,"end_time":"2023-07-25T16:18:13.415572","exception":false,"start_time":"2023-07-25T16:18:12.719132","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:33:23.185264Z","iopub.execute_input":"2023-09-10T23:33:23.185788Z","iopub.status.idle":"2023-09-10T23:33:23.194378Z","shell.execute_reply.started":"2023-09-10T23:33:23.185749Z","shell.execute_reply":"2023-09-10T23:33:23.193147Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"predicted_labels = []\npredicted_probs = []\nfor text in tqdm(test_texts):\n    predicted_label, prob = predict_labels(text)\n    predicted_labels.append(predicted_label)\n    predicted_probs.append(prob)\n\n# Calculate accuracy and F1 score\naccuracy = accuracy_score(test_labels, predicted_labels)\n# f1 = f1_score(test_labels, predicted_labels)\n# roc_auc = roc_auc_score(test_labels, predicted_probs)\n\nprint('Accuracy:', accuracy)\n# print('F1 Score:', f1)\n# print('ROC-AUC:', roc_auc)","metadata":{"id":"d57fe764","outputId":"d7e9495a-7096-413f-cc6e-0f3e69cdc327","papermill":{"duration":56.127329,"end_time":"2023-07-25T16:19:09.548653","exception":false,"start_time":"2023-07-25T16:18:13.421324","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:32:50.592424Z","iopub.execute_input":"2023-09-10T23:32:50.592779Z","iopub.status.idle":"2023-09-10T23:33:23.143145Z","shell.execute_reply.started":"2023-09-10T23:32:50.592743Z","shell.execute_reply":"2023-09-10T23:33:23.141981Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2010 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d89ef1d88e24660a7b9120244b7bbd8"}},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.7960199004975125\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Accuracy:', accuracy)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:33:23.144578Z","iopub.execute_input":"2023-09-10T23:33:23.145028Z","iopub.status.idle":"2023-09-10T23:33:23.151580Z","shell.execute_reply.started":"2023-09-10T23:33:23.144992Z","shell.execute_reply":"2023-09-10T23:33:23.150611Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy: 0.7960199004975125\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, classification_report\n\nprint('\\nThe Classification Report is as follows\\n')\nprint(classification_report(test_labels, predicted_labels, digits = 4))","metadata":{"papermill":{"duration":0.031899,"end_time":"2023-07-25T16:19:09.586107","exception":false,"start_time":"2023-07-25T16:19:09.554208","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-10T23:33:23.153681Z","iopub.execute_input":"2023-09-10T23:33:23.154408Z","iopub.status.idle":"2023-09-10T23:33:23.182498Z","shell.execute_reply.started":"2023-09-10T23:33:23.154368Z","shell.execute_reply":"2023-09-10T23:33:23.181477Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nThe Classification Report is as follows\n\n              precision    recall  f1-score   support\n\n           0     0.7896    0.8843    0.8343      1167\n           1     0.8080    0.6738    0.7348       843\n\n    accuracy                         0.7960      2010\n   macro avg     0.7988    0.7791    0.7845      2010\nweighted avg     0.7973    0.7960    0.7926      2010\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}